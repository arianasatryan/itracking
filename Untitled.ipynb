{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20deec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "models = {\n",
    "    \"face\": YOLO('models/yolov8n-face.pt'),\n",
    "    \"pose\": YOLO(f\"yolov8x-pose.pt\")\n",
    "}\n",
    "\n",
    "\n",
    "def detect(image_path, category, stream=True, show_keypoints=True, show_boxes=True):\n",
    "    assert category in models\n",
    "    model = models[category]\n",
    "    results = model.predict(source=image_path, conf=0.3, classes=0, stream=stream)\n",
    "    if not show_keypoints and not show_boxes:\n",
    "        return None, results\n",
    "    img = cv2.imread(image_path)\n",
    "    for result in results:\n",
    "        if show_keypoints:\n",
    "            for object_keypoints in result.keypoints:\n",
    "                for sublist in object_keypoints.xy:\n",
    "                    for point in sublist.int().tolist():\n",
    "                        cv2.circle(img, tuple(point), 3, (0, 0, 255), -1)\n",
    "        if show_boxes:\n",
    "            for object_boxes in result.boxes:\n",
    "                for xmin, ymin, xmax, ymax in object_boxes.xyxy.int().tolist():\n",
    "                    cv2.rectangle(img, (xmin, ymin),\n",
    "                                  (xmax, ymax), (0, 255, 0), 2)\n",
    "    return img, results\n",
    "\n",
    "\n",
    "def track(video_path, category, stream=True, show_keypoints=True, show_boxes=True):\n",
    "    assert category in models\n",
    "    model = models[category]\n",
    "    results = model.track(source=video_path, conf=0.3, classes=0, stream=stream)\n",
    "    if not show_keypoints and not show_boxes:\n",
    "        return None, results\n",
    "    img = cv2.imread(video_path)\n",
    "    for result in results:\n",
    "        if show_keypoints:\n",
    "            for object_keypoints in result.keypoints:\n",
    "                for sublist in object_keypoints.xy:\n",
    "                    for point in sublist.int().tolist():\n",
    "                        cv2.circle(img, tuple(point), 3, (0, 0, 255), -1)\n",
    "        if show_boxes:\n",
    "            for object_boxes in result.boxes:\n",
    "                for xmin, ymin, xmax, ymax in object_boxes.xyxy.int().tolist():\n",
    "                    cv2.rectangle(img, (xmin, ymin),\n",
    "                                  (xmax, ymax), (0, 255, 0), 2)\n",
    "    return img, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "output_video_path = 'output_video1.mp4'\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        results = model.track(frame, persist=True)\n",
    "        annotated_frame = results[0].plot()\n",
    "        out.write(annotated_frame)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d03b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/lab6/Desktop/new/itracking/assets/files/face-example-2.jpg: 448x640 10 persons, 2300.1ms\n",
      "Speed: 3.0ms preprocess, 2300.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "models = {\n",
    "    \"face\": YOLO('models/yolov8n-face.pt'),\n",
    "    \"pose\": YOLO(f\"yolov8x-pose.pt\")\n",
    "}\n",
    "\n",
    "category = 'pose'\n",
    "image_path = '/home/lab6/Desktop/new/itracking/assets/files/face-example-2.jpg'\n",
    "\n",
    "model = models[category]\n",
    "results = model.predict(source=image_path, conf=0.3, classes=0, stream=False)\n",
    "annotated_frame = results[0].plot(boxes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a83542f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[241, 230, 232],\n",
       "        [241, 230, 232],\n",
       "        [241, 230, 232],\n",
       "        ...,\n",
       "        [255, 250, 239],\n",
       "        [255, 249, 238],\n",
       "        [255, 249, 238]],\n",
       "\n",
       "       [[241, 230, 232],\n",
       "        [241, 230, 232],\n",
       "        [241, 230, 232],\n",
       "        ...,\n",
       "        [255, 250, 239],\n",
       "        [255, 249, 238],\n",
       "        [255, 249, 238]],\n",
       "\n",
       "       [[241, 230, 232],\n",
       "        [241, 230, 232],\n",
       "        [241, 230, 232],\n",
       "        ...,\n",
       "        [255, 250, 239],\n",
       "        [255, 250, 239],\n",
       "        [255, 249, 238]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 47,  38,  25],\n",
       "        [ 36,  27,  17],\n",
       "        [ 25,  16,   6],\n",
       "        ...,\n",
       "        [ 58,  56, 115],\n",
       "        [ 61,  59, 118],\n",
       "        [ 63,  61, 120]],\n",
       "\n",
       "       [[ 36,  27,  14],\n",
       "        [ 35,  26,  13],\n",
       "        [ 32,  23,  13],\n",
       "        ...,\n",
       "        [ 54,  54, 108],\n",
       "        [ 54,  54, 108],\n",
       "        [ 54,  54, 108]],\n",
       "\n",
       "       [[ 26,  15,   1],\n",
       "        [ 32,  23,  10],\n",
       "        [ 40,  31,  18],\n",
       "        ...,\n",
       "        [ 51,  52, 103],\n",
       "        [ 50,  51, 102],\n",
       "        [ 49,  50, 101]]], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].plot(boxes=False, kpt_radius=3, filename='output_image.jpg', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3913ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opencv_image = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite('output_image.jpg', opencv_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dce29af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image_path, category, output_path, show_keypoints=True, show_boxes=True):\n",
    "    assert category in models\n",
    "    model = models[category]\n",
    "    results = model.predict(source=image_path, conf=0.3, classes=0, stream=False, verbose=False)\n",
    "    pil_image = results[0].plot(boxes=show_boxes, kpt_radius=int(show_keypoints)*3, pil=True, save=True, filename=output_path)\n",
    "    return pil_image, results\n",
    "\n",
    "\n",
    "def track(video_path, category, output_video_path, show_keypoints=True, show_boxes=True):\n",
    "    assert category in models\n",
    "    model = models[category]\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            results = model.track(frame, persist=True)\n",
    "            annotated_frame = results[0].plot(boxes=show_boxes, kpt_radius=int(show_keypoints)*3)\n",
    "            out.write(annotated_frame)\n",
    "        else:\n",
    "            break\n",
    "    return output_video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47c3be3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pose-track-example1.mp4'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filename = 'pose-track-example1.gif'\n",
    "path = f'/home/lab6/Desktop/new/itracking/assets/files/{filename}'\n",
    "output_video_path = filename.split('.')[0] + '.mp4'\n",
    "category = 'pose'\n",
    "\n",
    "# # detect(path, category, output_path=f\"processed_{filename}\")\n",
    "track(path, category, output_video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
